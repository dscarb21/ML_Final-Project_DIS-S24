{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899e17ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(5)\n",
    "\n",
    "# Read in IMDb and Wikipedia movie data (both in same file)\n",
    "movies_df = pd.read_csv('netflix_shows.csv')\n",
    "\n",
    "nltk.download('punkt')\n",
    "# Tokenize a paragraph into sentences and store in sent_tokenized\n",
    "sent_tokenized = [sent for sent in nltk.sent_tokenize(\"\"\"\n",
    "                        Today (May 19, 2016) is his only daughter's wedding. \n",
    "                        Vito Corleone is the Godfather.\n",
    "                        \"\"\")]\n",
    "\n",
    "# Word Tokenize first sentence from sent_tokenized, save as words_tokenized\n",
    "words_tokenized = [word for word in nltk.word_tokenize(sent_tokenized[0])]\n",
    "\n",
    "# Remove tokens that do not contain any letters from words_tokenized\n",
    "import re\n",
    "\n",
    "filtered = [word for word in words_tokenized if re.search('[a-zA-Z]', word)]\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "# Initialize the Snowball stemmer for English\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "\n",
    "stemmed_words = [snowball_stemmer.stem(word) for word in filtered]\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    sent_tokenized = [sent for sent in nltk.sent_tokenize(text)]\n",
    "    words_tokenized = [word for word in nltk.word_tokenize(sent_tokenized[0])]\n",
    "    filtered = [word for word in words_tokenized if re.search('[a-zA-Z]', word)]\n",
    "    stemmed_words = [snowball_stemmer.stem(word) for word in filtered]\n",
    "    return stemmed_words\n",
    "\n",
    "\n",
    "from tfIdfInheritVectorizer.feature_extraction.vectorizer import TFIDFVectorizer as TfidfVectorizer\n",
    "# Might want to switch to sklearn here? Has way more documentation\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=1.0, max_features=200000,\n",
    "                                 min_df=0.0, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem,\n",
    "                                 ngram_range=(1,3))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform([x for x in movies_df[\"description\"]])\n",
    "\n",
    "# Import k-means to perform clusters\n",
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=4)\n",
    "\n",
    "km.fit(tfidf_matrix)\n",
    "clusters = km.labels_.tolist()\n",
    "movies_df[\"cluster\"] = clusters\n",
    "cluster_counts = movies_df['cluster'].value_counts()\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Calculate the similarity distance\n",
    "similarity_distance = 1 - cosine_similarity(tfidf_matrix)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "cosine_sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "# Exclude self-similarity comparisons (do not compare a document to itself)\n",
    "np.fill_diagonal(cosine_sim_matrix, 0)\n",
    "# Get indices for top right of matrix (except main diagonal)\n",
    "upper_triangular_indices = np.triu_indices(cosine_sim_matrix.shape[0], k=1)\n",
    "# Get according similarities values generated previously\n",
    "similarities_flat = cosine_sim_matrix[upper_triangular_indices]\n",
    "#Sort to get top 20 most similar shows\n",
    "top_20_indices = similarities_flat.argsort()[-20:][::-1]\n",
    "\n",
    "most_similar_indices = (upper_triangular_indices[0][top_20_indices], upper_triangular_indices[1][top_20_indices])\n",
    "\n",
    "for i, (idx1, idx2) in enumerate(zip(*most_similar_indices), 1):\n",
    "    movie1_title = movies_df.iloc[idx1][\"title\"]\n",
    "    movie2_title = movies_df.iloc[idx2][\"title\"]\n",
    "    print(f\"{i}. {movie1_title} and {movie2_title}\")\n",
    "\n",
    "def find_similar_movies(input_description):\n",
    "    # Preprocess input description\n",
    "    input_description_stemmed = tokenize_and_stem(input_description)\n",
    "    input_description_transformed = ' '.join(input_description_stemmed)\n",
    "    \n",
    "    # Transform input description into TF-IDF vector\n",
    "    input_tfidf_vector = tfidf_vectorizer.transform([input_description_transformed])\n",
    "    \n",
    "    # Calculate cosine similarity between input description and dataset descriptions\n",
    "    similarity_scores = cosine_similarity(input_tfidf_vector, tfidf_matrix)\n",
    "    \n",
    "    # Get indices of top similar movies\n",
    "    top_indices = similarity_scores.argsort()[0][-5:][::-1]\n",
    "    \n",
    "    # Get titles and descriptions of top similar movies\n",
    "    similar_movies_data = []\n",
    "    for idx in top_indices:\n",
    "        title = movies_df.iloc[idx]['title']\n",
    "        description = movies_df.iloc[idx]['description']\n",
    "        similar_movies_data.append((title, description))\n",
    "    \n",
    "    \n",
    "    return similar_movies_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9c9260",
   "metadata": {},
   "outputs": [],
   "source": [
    "## K-means plot\n",
    "\n",
    "# Import k-means to perform clusters\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create a KMeans object with 4 clusters and save as km\n",
    "km = KMeans(n_clusters=4)\n",
    "\n",
    "# Fit the k-means object with tfidf_matrix\n",
    "km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "# Create a column cluster to denote the generated cluster for each movie\n",
    "df[\"cluster\"] = clusters\n",
    "\n",
    "# Display number of films per cluster (clusters from 0 to 4)\n",
    "df['cluster'].value_counts() \n",
    "\n",
    "#fewer points for better plot\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform([x for x in df['description'][0:500]])\n",
    "\n",
    "# Create a KMeans object with 4 clusters and save as km\n",
    "km = KMeans(n_clusters=4)\n",
    "\n",
    "# Fit the k-means object with tfidf_matrix\n",
    "km.fit(tfidf_matrix)\n",
    "\n",
    "y_kmeans = km.predict(tfidf_matrix)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "kmean_indices = km.fit_predict(tfidf_matrix)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "scatter_plot_points = pca.fit_transform(tfidf_matrix.toarray())\n",
    "\n",
    "colors = [\"red\", \"blue\", \"olive\"]\n",
    "\n",
    "x_axis = [o[0] for o in scatter_plot_points]\n",
    "y_axis = [o[1] for o in scatter_plot_points]\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "\n",
    "ax.scatter(x_axis, y_axis, c=[colors[d] for d in kmean_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b076026",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Silhouette Score plot\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sil = []\n",
    "k_range = range(2,8)\n",
    "for k in k_range :\n",
    "  kmeans = KMeans(n_clusters = k).fit(tfidf_matrix)\n",
    "  labels = kmeans.predict(tfidf_matrix)\n",
    "  sil.append(silhouette_score(tfidf_matrix, labels))\n",
    "\n",
    "print(len(sil))\n",
    "plt.plot(range(2,8),np.array(sil))\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Silhouette index\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0b7711",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SSE plot\n",
    "\n",
    "sse_scaler  = []\n",
    "for k in range(2,8):\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km.fit(tfidf_matrix)\n",
    "    km.predict(tfidf_matrix)\n",
    "    sse_scaler.append(km.inertia_)\n",
    "plt.plot(range(2,8),sse_scaler)\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Sum of squared error\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f204f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pie Charts\n",
    "\n",
    "visualization_df = pd.DataFrame(columns=[])\n",
    "\n",
    "for index, movie in movies_df.iterrows():\n",
    "    cluster = movie[9]\n",
    "    if cluster not in visualization_df.columns:\n",
    "        visualization_df[cluster] = ''\n",
    "        visualization_df.loc[len(visualization_df)] = {}\n",
    "        genres = movie[7].split(\", \")\n",
    "        for genre in genres:\n",
    "            visualization_df.at[len(visualization_df)-1, cluster] = genre\n",
    "    else:\n",
    "        visualization_df.loc[len(visualization_df)] = {}\n",
    "        genres = movie[7].split(\", \")\n",
    "        for genre in genres:\n",
    "            visualization_df.at[len(visualization_df)-1, cluster] = genre\n",
    "\n",
    "my_labels = []\n",
    "total_genres = list(visualization_df[0]) + list(visualization_df[1]) + list(visualization_df[2]) + list(visualization_df[3])\n",
    "total_genres = [x for x in total_genres if (str(x) != 'nan' and str(x) != \"\")]\n",
    "genres = list(set(total_genres))\n",
    "for genre in genres:\n",
    "    my_labels.append(genre)\n",
    "            \n",
    "for cluster in visualization_df.columns:\n",
    "    counts = []\n",
    "    print(cluster)\n",
    "    total_genres = list(visualization_df[cluster])\n",
    "    total_genres = [x for x in total_genres if (str(x) != 'nan' and str(x) != \"\")]\n",
    "    genres = list(set(total_genres))\n",
    "    for label in my_labels:\n",
    "        counts.append(total_genres.count(label))\n",
    "    plt.pie(counts, labels = my_labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038b7e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
